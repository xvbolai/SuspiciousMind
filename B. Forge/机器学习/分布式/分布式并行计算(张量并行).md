## 分布式计算总览

大语言模型时代，单个 GPU 的显存可能连模型参数都放不下，更不用提训练和微调了。分布式计算不可或缺。一方面可以减轻单个 GPU 的显存压力；另一方面，通过并行计算可以加速训练。

分布式计算的方法可以分为三类：

- **数据并行（Data Parallelism）**：在不同 GPU 上运行不同的 batch data；[ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1910.02054) 这篇文章就是基于数据并行，目前 Deepspeed 库以及 Pytorch 的 FSDP 都是基于 ZeRO 的思路实现的。
- **张量并行（Tensor Parallelism）**：将单个数学运算（如矩阵乘法）拆分到不同的 GPU 上运行；代表论文：[Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1909.08053)
- **流水线并行（Pipeline Parallelism）**：在不同 GPU 上运行模型的不同层；代表论文：[GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1811.06965)

这篇文章就来研究一下 **Megatron-LM** 这篇论文以及它背后的张量并行

---

## Megatron-LM

虽然文章的标题是 Model Parallelism，导致后面一些工作沿用了“模型并行”这个名字，但其实“张量并行”更贴合它的思路。

Megatron-LM 提出了一种**针对 Transformer** 结构的**层内切分**方式，可以把每一层的参数切分放在不同 GPU 上。

具体来说，Transformer 结构可以分成三种 block：MLP，self attention layer 以及 embedding layer (& output layer)。下面看看针对这三种 block，Megatron-LM 如何设计切分模式

### MLP

Transformer 中 MLP 的输入 X 是三维张量 b*l*k，分别代表 batch size, sequence length, hidden dimension.

为了方便起见，我们把 X 表示成二维向量 (b*l)*k

下图中 A 和 B 表示 MLP block 中的两个全连接层的矩阵，不同的颜色代表它们分块储存在不同 GPU 上（图中以两块 GPU 为例）

![[Pasted image 20230718133359.png]]

每个 GPU 节点都要维护一份完整的输入 X。对于**矩阵 A，按照纵向切分**；因此 XA 的不同的列储存在不同 GPU 上。对于**矩阵 B，按照横向切分**，这样一来每个 GPU 得到的输出 Y 的形状都是一样的，利用 All Reduce 在各个 GPU 间通信（即把各个 GPU 的结果 Y 相加，把结果返回给各个 GPU）得到完整的输出 Y

通信成本：前向传播时，只需要对 Y 进行 All Reduce，通信成本为 **b\*l\*k。**反向传播时，需要对梯度进行 All Reduce，通信成本是一样的。

![[Pasted image 20230718133411.png]]

复习一下 All Reduce

此时再看论文中的示意图就清晰多了，并且对反向传播时的步骤也会理解得会更好：
![[Pasted image 20230718133440.png]]

前向传播时，f 什么都不干；g 代表 all reduce；反向传播时刚好相反

---

### self attention

对于自注意力层，它是按照 attention head 进行切分的。把 attention heads 平均分配到每个 GPU 上

![[Pasted image 20230718133507.png]]

上图表现了两个 attention heads 分配到两个 GPU 上。实际中有可能是八个 attention heads 分配到两个 GPU 上，每个 GPU 上四个 heads，不过原理是一样的。

通信成本：前向传播时，与MLP 一样，只需要对 Y 进行 All Reduce，通信成本为 **b\*l\*k**

![[Pasted image 20230718133519.png]]

前向传播时，f 什么都不干；g 代表 all reduce；反向传播时刚好相反

---

### embedding layer & output layer

对于 embedding layer，在 V（词库大小）维度上进行切分。输入 tokens 分别在各个 GPU 上“查字典”。如果该 GPU 上维护了这个 token 对应的向量，那么记录该向量；如果没有维护，结果就是 0.

最后把各个 GPU 的结果进行 All Reduce，每个 GPU 就都得到了所有 tokens 的向量表示

![[Pasted image 20230718133539.png]]

通信成本：依旧是 **b\*l\*k**

语言模型中，output layer 一般与 embedding layer 共享参数。

![[Pasted image 20230718133552.png]]

值得注意的是，为了节约通讯成本，我们不对 Y 直接进行 All Reduce，因为 V 太大了，通常能够达到几万。由于之后要计算 softmax 得到概率，所以先在每个 GPU 上计算指数，并且按照行加和，对该结果进行 All Reduce。

这一步的通信成本只有 **b\*l**

---

## 总结

![[Pasted image 20230718133603.png|center]]


Megatron-LM 针对 Transformer 结构设计了张量并行。正如上图所示，不管是哪一种 block（MLP，自注意力或者是 embedding 层）它都是把层内的参数平均切分到各个 GPU 单元上，并且在每一层都需要通过 All Reduce 获取完整的输出。

可以看出 Megatron-LM 的几个不足之处：

1. 计算和通讯不能并行：必须等待该层的 All Reduce 通讯完成之后，才能开始下一层的计算。对于带宽的要求很高，否则性能会急剧下降  
    论文中实验所用的机器，同一机器内 GPU 的通讯带宽能达到 300 GB/s；不同机器间 GPU 通讯带宽也有 100 GB/s. 这是非常豪华的机器配置。
2. 尽管每个 GPU 只保留每层中的一部分参数，但是它要保存每一层完整的输入，这一部分的内存占用也不小
3. 张量的切分方式专为 Transformer 结构设计，具有较强的专用性